{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoliTwit\n",
    "\n",
    "## Examining political speech on social media\n",
    "\n",
    "### Tyler Nevell and Kyle Staub\n",
    "\n",
    "### GitHub link: https://github.com/KStaub/politwit\n",
    "\n",
    "#### Dataset\n",
    "\n",
    "* Origin: \n",
    "    * Crowdflower Data for Everyone dataset: https://www.crowdflower.com/data-for-everyone/\n",
    "    * https://www.kaggle.com/crowdflower/political-social-media-posts/data\n",
    "* Meta-data:\n",
    "    * _unit_id: a unique id for the message\n",
    "    * _golden: always FALSE; (presumably whether the message was in Crowdflower's gold standard)\n",
    "    * _unit_state: always \"finalized\"\n",
    "    * _trusted_judgments: the number of trusted human judgments that were entered for this message; an integer between 1 and 3\n",
    "    * _last_judgment_at: when the final judgment was collected\n",
    "    * audience: one of national or constituency\n",
    "    * audience:confidence: a measure of confidence in the audience judgment; a float between 0.5 and 1\n",
    "    * bias: one of neutral or partisan\n",
    "    * bias:confidence: a measure of confidence in the bias judgment; a float between 0.5 and 1\n",
    "    * message: the aim of the message. one of: \n",
    "        * attack: the message attacks another politician \n",
    "        * constituency: the message discusses the politician's constituency \n",
    "        * information: an informational message about news in government or the wider U.S. \n",
    "        * media: a message about interaction with the media \n",
    "        * mobilization: a message intended to mobilize supporters \n",
    "        * other: a catch-all category for messages that don't fit into the other \n",
    "        * personal: a personal message, usually expressing sympathy, support or condolences, or other personal opinions \n",
    "        * policy: a message about political policy \n",
    "        * support: a message of political support\n",
    "    * message:confidence: a measure of confidence in the message judgment; a float between 0.5 and 1\n",
    "    * orig__golden: always empty; presumably whether some portion of the message was in the gold standard\n",
    "    * audience_gold: always empty; presumably whether the audience response was in the gold standard\n",
    "    * bias_gold: always empty; presumably whether the bias response was in the gold standard\n",
    "    * bioid: a unique id for the politician\n",
    "    * embed: HTML code to embed this message\n",
    "    * id: unique id for the message WITHIN whichever social media site it was pulled from\n",
    "    * label: a string of the form \"From: firstname lastname (position from state)\"\n",
    "    * message_gold: always blank; presumably whether the message response was in the gold standard\n",
    "    * source: where the message was posted; one of \"facebook\" or \"twitter\"\n",
    "    * text: the text of the message\n",
    "    \n",
    "#### Goal\n",
    "\n",
    "Seeking to predict relationships between certain words and certain message types as determined by qualified human assessors. To do so, we will apply both a Naive Bayes (NB) Classifier and a Support Vector Machine (SVM).  \n",
    "\n",
    "- <b>Hypothesis \\#1</b>: Our SVM classifier will be a better predictor than our NB classifier.\n",
    "- <b>Hypothesis \\#2</b>: Certain words will be highly correlated with certain messages. For instance, we anticipate the following text/message pairs to be highly correlated:\n",
    "    * Obama > attack\n",
    "    * veterans > support\n",
    "    * proud > support\n",
    "    * bill > policy\n",
    "    * ICYMI > media\n",
    "\n",
    "#### Data provenance and trustworthiness considerations\n",
    "\n",
    "This dataset contains 5000 entries of tweets or Facebook posts by members of Congress, that are labelled by \"contributors\". Contributors from Crowdflower (https://www.crowdflower.com/data-for-everyone-full-library/#!/) go through each post and classify its contents on the basis of the audience, bias, and message components listed in the metadata. It is an interesting dataset because it contains confidence measures, but is also subject to persuasion by the contributors themselves, pointing to a central problem in trusting human labeled datasets. We believe that this dataset will largely conform with the average person's intuitions about the labelling of tweets according to a binary statement of bias, or a categorical attribution of message type. \n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "For the purposes of our scope, many of the columns listed in the metadata are superfluous. We will be using only four columns: Message, Bias, Label, and Text. To be fed into our classifiers, text will be tokenized into a numeric representation of the text, which we can use to predict a certain message type as one of: \n",
    "* attack\n",
    "* constituency\n",
    "* information\n",
    "* media\n",
    "* mobilization \n",
    "* other \n",
    "* personal \n",
    "* policy \n",
    "* support\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.classifiers import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import sklearn.linear_model\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.neighbors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt = pd.read_csv('political_statements_raw.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>wordvec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>From: C.A. Dutch Ruppersberger (Representative...</td>\n",
       "      <td>.@DeltaDiva3 One idea is to allow students to ...</td>\n",
       "      <td>[.@DeltaDiva3, One, idea, is, to, allow, stude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>media</td>\n",
       "      <td>From: Jeb Hensarling (Representative from Texas)</td>\n",
       "      <td>Joining the @MarkDavis show on @660KSKY this m...</td>\n",
       "      <td>[Joining, the, @MarkDavis, show, on, @660KSKY,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>information</td>\n",
       "      <td>From: Cory Booker (Senator from New Jersey)</td>\n",
       "      <td>RT @HeraldNews See photos from the ‰Û÷Run With...</td>\n",
       "      <td>[RT, @HeraldNews, See, photos, from, the, ‰Û÷R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>From: John McCain (Senator from Arizona)</td>\n",
       "      <td>Headed to #Scottsdale for event honoring Marsh...</td>\n",
       "      <td>[Headed, to, #Scottsdale, for, event, honoring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>information</td>\n",
       "      <td>From: JosÌ© Serrano (Representative from New Y...</td>\n",
       "      <td>Today at 10:30 ribbon cutting ceremony at @sou...</td>\n",
       "      <td>[Today, at, 10:30, ribbon, cutting, ceremony, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bias      message                                              label  \\\n",
       "0  partisan       policy  From: C.A. Dutch Ruppersberger (Representative...   \n",
       "1   neutral        media   From: Jeb Hensarling (Representative from Texas)   \n",
       "2   neutral  information        From: Cory Booker (Senator from New Jersey)   \n",
       "3   neutral      support           From: John McCain (Senator from Arizona)   \n",
       "4   neutral  information  From: JosÌ© Serrano (Representative from New Y...   \n",
       "\n",
       "                                                text  \\\n",
       "0  .@DeltaDiva3 One idea is to allow students to ...   \n",
       "1  Joining the @MarkDavis show on @660KSKY this m...   \n",
       "2  RT @HeraldNews See photos from the ‰Û÷Run With...   \n",
       "3  Headed to #Scottsdale for event honoring Marsh...   \n",
       "4  Today at 10:30 ribbon cutting ceremony at @sou...   \n",
       "\n",
       "                                             wordvec  \n",
       "0  [.@DeltaDiva3, One, idea, is, to, allow, stude...  \n",
       "1  [Joining, the, @MarkDavis, show, on, @660KSKY,...  \n",
       "2  [RT, @HeraldNews, See, photos, from, the, ‰Û÷R...  \n",
       "3  [Headed, to, #Scottsdale, for, event, honoring...  \n",
       "4  [Today, at, 10:30, ribbon, cutting, ceremony, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing columns irrelevant to the current analysis\n",
    "tdf = twt.drop(['_unit_id', '_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at', 'audience', \\\n",
    "                'audience:confidence', 'bias:confidence', 'message:confidence', 'orig__golden', 'audience_gold', \\\n",
    "               'bias_gold', 'bioid', 'embed', 'id', 'message_gold', 'source'], axis=1)\n",
    "\n",
    "# Creating a new column, wordvec, containing a vector of all words in the text field\n",
    "tdf['wordvec'] = [i for i in tdf.text.str.split(\" \")]\n",
    "\n",
    "# Converting the bias and message fields to categorical\n",
    "tdf.bias = pd.Categorical(tdf.bias)\n",
    "tdf.message = pd.Categorical(tdf.message)\n",
    "\n",
    "# Taking an initial look at our dataframe\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 most common words:\n",
      " [('the', 6979), ('to', 5844), ('and', 3604), ('of', 3360), ('in', 2712), ('a', 2384), ('', 2250), ('for', 2044), ('on', 1657), ('I', 1407), ('is', 1356), ('with', 1098), ('that', 1096), ('our', 1061), ('at', 979), ('this', 920), ('my', 740), ('be', 707), ('will', 689), ('are', 669), ('The', 660), ('you', 659), ('from', 650), ('by', 604), ('have', 569), ('about', 548), ('we', 531), ('their', 508), ('as', 452), ('was', 446), ('who', 428), ('it', 411), ('&amp;', 409), ('more', 408), ('House', 397), ('has', 379), ('an', 362), ('can', 354), ('all', 335), ('your', 334), ('not', 317), ('This', 296), ('his', 293), ('today', 291), ('We', 274), ('out', 253), ('they', 249), ('-', 247), ('or', 238), ('great', 229)]\n",
      "\n",
      "\n",
      "\n",
      "50 of the least common words:\n",
      " [('\"Public', 1), ('Servants', 1), ('Dinner.\"', 1), ('Parenting', 1), ('Galveston', 1), ('‰ÛÏEvery', 1), ('child,', 1), ('aborted,', 1), ('Lord,', 1), ('birth,', 1), ('born,', 1), ('rejection', 1), ('world.‰Û\\x9d-', 1), ('interesting.', 1), ('Index', 1), ('reception,', 1), ('beyond.', 1), ('hearty', 1), ('applauds', 1), ('ranger', 1), ('memorial.', 1), ('terror.', 1), ('Brenda', 1), ('commits', 1), ('Organizations', 1), ('(VSO).', 1), ('14?', 1), ('slavery.', 1), ('420', 1), ('95%,', 1), ('Include:', 1), ('*Provide', 1), ('*Ability', 1), ('apples', 1), ('*Independent,', 1), ('*Reduces', 1), ('*Increases', 1), ('last.', 1), ('River.', 1), ('Hemmer', 1), ('10:25', 1), ('Monica', 1), ('filmed', 1), ('Ensuring', 1), ('https://www.youtube.com/watch?v=2jE7kzv5hzc', 1), ('exceeds', 1), ('dependents', 1), ('in-state', 1), ('Post-9/11GI', 1), ('items', 1)]\n"
     ]
    }
   ],
   "source": [
    "# What are all of the unique words in wordvec?\n",
    "allwords = Counter()\n",
    "tdf.wordvec.apply(allwords.update)\n",
    "print('150 most common words:\\n', allwords.most_common(50))\n",
    "print('\\n\\n')\n",
    "print('50 of the least common words:\\n', allwords.most_common()[-250:-200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are some very clear outliers in this data, as you would suspect. The grammatical connective words ('the', 'to', 'and', etc.) are extremely common. Similarly, there are some outright unique words, largely due to an one of an encoding issue, hyperlink, etc. We intend to monitor this skew in the word frequency data for possible correction in the future. \n",
    "\n",
    "Further quantitative analysis isn't possible given the nature of our data. Because it is a set of discrete features (words in our tweets) attempting to classify a set of categorical resuts (message types), there is not a coherent distribution function that maps to our data. Similarly, correlation coefficients and r-squared values are inappropriate for describing our data. Rather, a frequency analysis (below) and an identification of the model's informative featureset (analogous to correlation coefficients) is more appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing our Correlation Hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tdf.message.unique()\n",
    "y = tdf.message.value_counts()\n",
    "plt.bar(x, y);\n",
    "plt.xticks(rotation=45);\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that we may have some problem due the heavy skew towards a few message types. Our hypotheses rely somewhat on an assumption of uniformity in the occurance of the different message types across our dataset. Due to this skew, our predicted messages for our selected words may be unlikely or may have a bias towards the high frequency categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All messages:\n",
      " count       5000\n",
      "unique         9\n",
      "top       policy\n",
      "freq        1411\n",
      "Name: message, dtype: object\n",
      "\n",
      "All biases:\n",
      " count        5000\n",
      "unique          2\n",
      "top       neutral\n",
      "freq         3689\n",
      "Name: bias, dtype: object\n",
      "\n",
      "For tweets containing the word 'Obama':\n",
      " count        405\n",
      "unique         9\n",
      "top       policy\n",
      "freq         185\n",
      "Name: message, dtype: object\n",
      "\n",
      "For tweets containing the word 'veterans':\n",
      " count        118\n",
      "unique         9\n",
      "top       policy\n",
      "freq          37\n",
      "Name: message, dtype: object\n",
      "\n",
      "For tweets containing the word 'proud':\n",
      " count          112\n",
      "unique           9\n",
      "top       personal\n",
      "freq            43\n",
      "Name: message, dtype: object\n",
      "\n",
      "For tweets containing the word 'bill':\n",
      " count        320\n",
      "unique         9\n",
      "top       policy\n",
      "freq         170\n",
      "Name: message, dtype: object\n",
      "\n",
      "For tweets containing the word 'ICYMI':\n",
      " count         71\n",
      "unique         6\n",
      "top       policy\n",
      "freq          23\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"All messages:\\n\",tdf.message.describe())\n",
    "print(\"\\nAll biases:\\n\",tdf.bias.describe())\n",
    "\n",
    "\n",
    "print(\"\\nFor tweets containing the word 'Obama':\\n\",tdf.message[tdf.text.str.contains(\"Obama\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'veterans':\\n\",tdf.message[tdf.text.str.contains(\"veterans\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'proud':\\n\",tdf.message[tdf.text.str.contains(\"proud\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'bill':\\n\",tdf.message[tdf.text.str.contains(\"bill\")].describe())\n",
    "print(\"\\nFor tweets containing the word 'ICYMI':\\n\",tdf.message[tdf.text.str.contains(\"ICYMI\")].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show that our hypotheses are largely incorrect. Concerning our example word, 'bill', our hypothesis was correct, but it appears that it may have been correct for the wrong reason. The most frequent message type, \"policy\", occurs much more frequently than we assumed in our hypotheses, rendering Hypothesis \\#2 incorrect.\n",
    "\n",
    "Furthermore, for any prediction about the relationship between certains words and their message types, those predictions are more likely to be accurate if they predict the message as policy, media, or information. This type of data skew is something that machine learning models are susceptible to replicating in making the laziest prediction. We will have to keep an eye on that as we apply our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming the Text and Splitting the Data\n",
    "\n",
    "In order for our classifiers to process the string data of our textyual fields, they need to be tokenized to numerical vectors. This process is different for our target and our data. For our target, because it is a categorical variable, we need to encode those categories into integers using a Label Encoder. For our data, because it is a set of discrete strings, we need to tokenize them on the basis of the frequency of their occurance in the corpus of text. \n",
    "\n",
    "After transforming our target and data, we need to split our training and testing data. We will use a standard test size of 33% of the dataset across our models, as this value seems to be semi-optimal for increasing our model's accuracy. With smaller test sizes, you may get higher point-accuracies for certain runs, purely due to coincidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our label encoder\n",
    "le = sklearn.preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our target categories are:  [0 1 2 3 4 5 6 7 8]\n",
      "which correspond to the following message types:  ['attack' 'constituency' 'information' 'media' 'mobilization' 'other'\n",
      " 'personal' 'policy' 'support']\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "# Our data is the text of the tweet, and the target is the assigned message\n",
    "X, y = tdf.text, tdf.message\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "# Tokenizing our sentences in order to pass them in as numerical values to our classifier\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(X)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "X = X_tfidf\n",
    "\n",
    "# Splitting our data into training and testing sets \n",
    "trainX, testX, trainy, testy = sklearn.model_selection.train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Label encoding our target message\n",
    "print(\"Our target categories are: \",np.unique(y))\n",
    "print(\"which correspond to the following message types: \",np.unique(le.inverse_transform(y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classification\n",
    "\n",
    "Multinomial Naive Bayes is the first step in working with text data. It is a supervised learning technique that uses Bayes Theorem with the assumption of independence between all of the features. We are using it here to classify certain tweets as a certain message type. We do the standard fit/predict steps to obtain our model and predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing our model\n",
    "mnnb = MultinomialNB()\n",
    "# Fitting our NB classifier\n",
    "classifier = mnnb.fit(trainX, trainy)\n",
    "# Creating a predictor for our test tweets\n",
    "predictor = classifier.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Multinomial NB classifier:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      policy       0.00      0.00      0.00        54\n",
      "       media       0.00      0.00      0.00        66\n",
      " information       0.00      0.00      0.00       217\n",
      "     support       0.00      0.00      0.00        78\n",
      " information       0.00      0.00      0.00        38\n",
      " information       0.00      0.00      0.00        39\n",
      "     support       0.52      0.42      0.46       400\n",
      "      policy       0.34      0.97      0.50       462\n",
      " information       0.14      0.00      0.01       296\n",
      "\n",
      " avg / total       0.25      0.37      0.25      1650\n",
      "\n",
      "Confusion matrix for Multinomial NB classifier:\n",
      "\n",
      " [[  0   0   0   0   0   0   0  54   0]\n",
      " [  0   0   0   0   0   0  21  45   0]\n",
      " [  0   0   0   0   0   0  35 181   1]\n",
      " [  0   0   0   0   0   0   9  68   1]\n",
      " [  0   0   0   0   0   0   6  32   0]\n",
      " [  0   0   0   0   0   0   6  33   0]\n",
      " [  0   0   0   0   0   0 169 229   2]\n",
      " [  0   0   0   0   0   0  13 447   2]\n",
      " [  0   0   0   0   0   0  68 227   1]]\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "print(\"Classification report for Multinomial NB classifier:\\n\\n\",metrics.classification_report(testy, predictor, target_names=le.inverse_transform(y)))\n",
    "print(\"Confusion matrix for Multinomial NB classifier:\\n\\n\",metrics.confusion_matrix(testy, predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of our Multinomial NB predictor is:  0.37393939393939396\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall accuracy of our Multinomial NB predictor is: \",np.mean(predictor == testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this appears to be a fairly successful model. While the classification report and the confusion matrix do show some serious overfitting to the overrepresented message types, we still achieve a fit that's better than pure chance, as discussed below. While the model is successful, it's confusion matrix shows serious deficiencies in classifying the majority of different messages types, as it takes the safe and lazy approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline\n",
    "X, y = tdf.text, tdf.message\n",
    "trainX, testX, trainy, testy = sklearn.model_selection.train_test_split(X, y, test_size=0.33)\n",
    "tweet_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='squared_loss', penalty='elasticnet',\n",
    "                                          alpha=1e-3, random_state=42,\n",
    "                                          max_iter=10, tol=None, \n",
    "                                          class_weight='balanced')),\n",
    "                     ]);\n",
    "\n",
    "# Fitting our classifier\n",
    "tweet_clf.fit(trainX, trainy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy of our Mulinomial NB predictor is:  0.3212121212121212\n"
     ]
    }
   ],
   "source": [
    "# Creating a predictor for our test tweets\n",
    "predicted = tweet_clf.predict(testX)\n",
    "print(\"Overall accuracy of our Mulinomial NB predictor is: \",np.mean(predicted == testy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for SVM classifier:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      attack       0.13      0.33      0.19        58\n",
      "constituency       0.08      0.19      0.11        52\n",
      " information       0.23      0.15      0.19       214\n",
      "       media       0.28      0.59      0.38        90\n",
      "mobilization       0.12      0.35      0.18        46\n",
      "       other       0.10      0.19      0.14        36\n",
      "    personal       0.50      0.39      0.44       404\n",
      "      policy       0.47      0.48      0.48       455\n",
      "     support       0.20      0.06      0.09       295\n",
      "\n",
      " avg / total       0.35      0.32      0.32      1650\n",
      "\n",
      "Confusion matrix for SVM classifier:\n",
      "\n",
      " [[ 19   0   4   2   6   3   0  22   2]\n",
      " [  3  10   6   5   1   4  11   6   6]\n",
      " [ 14  25  33  23  14   7  24  63  11]\n",
      " [  1   9   7  53   2   1   6   8   3]\n",
      " [  1   4   4   3  16   3   2  11   2]\n",
      " [  5   3   3   4   3   7   8   1   2]\n",
      " [ 15  41  34  36  26  20 156  54  22]\n",
      " [ 69  16  29  31  35   6  32 219  18]\n",
      " [ 15  16  22  32  25  16  72  80  17]]\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "print(\"Classification report for SVM classifier:\\n\\n\",metrics.classification_report(testy, predicted))\n",
    "print(\"Confusion matrix for SVM classifier:\\n\\n\",metrics.confusion_matrix(testy, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Models and Assessing Hypothesis \\#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 25 runs, our average accuracy is: 0.3185454545454546\n"
     ]
    }
   ],
   "source": [
    "runs = 25\n",
    "accuracies = np.empty(runs)\n",
    "for i in range(len(accuracies)):\n",
    "    trainX, testX, trainy, testy = sklearn.model_selection.train_test_split(X, y, test_size=0.33)\n",
    "    tweet_clf.fit(trainX, trainy);\n",
    "    pred = tweet_clf.predict(testX)\n",
    "    accuracies[i] = np.mean(pred == testy)\n",
    "    \n",
    "print('For {} runs, our average accuracy is: {}'.format(runs, np.mean(accuracies)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model to be better than chance, we have to achieve over 0.11 probability of making a correct prediction. If we chose a message at random, due to the skew of our data, the absolute *best* we could do is guess correctly 1 out of 9 times. \n",
    "\n",
    "For the afore-run Multinomial NB classifier, we consistently achieved between 0.20 and 0.40 prediction accuracy. This is certainly better than chance, though fairly inconsistent. One glaring problem with this model is its tendency to predict that ALL tweets are of two message types exclusively. That is the problem of laziness discussed above. Because the majority of tweets fall into just a few message categories, the classifer over-predicts for those categories because it is *more likely* to be correct.\n",
    "\n",
    "Given that our average accuracy over many runs for the SVM classifier is ~ 0.35, our model is performing pretty well. Given a larger data set and a more diverse set of labelled examples (where each message type has >5000 samples), our SVM model should be able to continue to improve. \n",
    "\n",
    "While our accuracy is not strictly greater than the accuracy achieved with Multinomial NB, we still conclude that Hypothesis \\#1 is correct and that the SVM is a better predictor than Multinomial NB. The reason for this lies in the confusion matrix. The SVM very clearly considers each message type as a possibility and is much less \"lazy\", in that it doesn't force each message to two or three likely categories of message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 most informative words (features) our model uses for each message type:\n",
      "\n",
      "\n",
      "attack: loan result his worst via pres interesting hand left or hours why uncertainty irs republicans president losing he obamacare obama\n",
      "\n",
      "\n",
      "constituency: lunch winchester 05 hall owners employees touring transit trade glad came attend office 2015 owned nc cmt_82 indiana town thinking\n",
      "\n",
      "\n",
      "information: employers had mlk annual caucus news www month korean voted in from hearing here 21 events academy attending find icymi\n",
      "\n",
      "\n",
      "media: foxnews here catch joining watch join speaking ukraine et ll at morning live talk interview cspan on listen show tune\n",
      "\n",
      "\n",
      "mobilization: trenton compromise 7gzb8ltbw7 favorite hear don nationaldayofprayer wilmington tnfarmbureau cancer facebook petition violence domestic please free percent follow kck share\n",
      "\n",
      "\n",
      "other: ice entries asawj2gynx ramallah olson ilntqltmx0 ca49 partying passport btw timscotthr obamascare biased newsletter protest gl goo kacoppola que sinema\n",
      "\n",
      "\n",
      "personal: game thank her enjoyed your weekend park enjoy prayers and remember friend my art honored congratulations thanks congrats day happy\n",
      "\n",
      "\n",
      "policy: why reform pay the defense under income nsa more need act it committee are health obamacare government not that bill\n",
      "\n",
      "\n",
      "support: strengthen fighting washington proud director life agree house bring general out reform amp gun help thanks see for thank support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def informative_features(vect, clf, class_labels):\n",
    "    feature_names = vect.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top10 = np.argsort(clf.coef_[i])[-20:]\n",
    "        print(\"\\n%s: %s\\n\" % (class_label,\n",
    "              \" \".join(feature_names[j] for j in top10)))\n",
    "\n",
    "print(\"The 20 most informative words (features) our model uses for each message type:\\n\")\n",
    "informative_features(tweet_clf.get_params()['vect'], tweet_clf.get_params()['clf'], np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you can see the most informative words that our model used to predict the message type for a given tweet. Glancing over this list, which is not static but rather changes each train/test split, many of these features make intuitive sense. For instance, in 'support' you see 'courage', in 'attack' you see 'askdems' and 'obamacare', in 'media' you see 'townhall', 'live', 'watch', 'interview', and 'tune'. You also see some clear examples of overfitting. If there is a single tweet with a unique word mapping to one message type, it is likely that the model will pick up on that and place heavy weight on that unique feature. Our features are certainly not entirely accurate, for instance 'protest' being an informative feature for 'other' and not for 'mobilization'. We have strong intuitions about certain words, like 'protest', that in turn map strongly to the message type we would designate. Therefore, we have to draw into question the decisions of our qualified human data-labellers, or the accuracy of our model. \n",
    "\n",
    "Outside of these concerns and those above, it appears that our model does about three times better than pure chance, and contains some fairly intuitive features, as we will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unseen Tweets and Further Possibilities\n",
    "\n",
    "Below, we hand-sample two tweets from four Colorado Congressional representatives to test our model on data not included in our dataset. A possible extension of this project is to hook up to the Twitter API and consume new tweets from each representative contained in our dataset to classify their live tweets. This API-hook is outside of the scope of this project, but we can still test our model on unclassified tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sen. Cory Gardner tweeted: \n",
      "ICYMI: Be sure to watch this @FOX21News clip on my recent trip to Afghanistan to visit our Colorado-based troops. I'm incredibly proud of the work these men and women do everyday.\n",
      "\n",
      " Predicted message type: personal\n",
      "----------------\n",
      "\n",
      "Sen. Cory Gardner tweeted: \n",
      "My job is to represent Colorado in the United States Senate and that means working across party lines to get things done for the state. I'm proud to be ranked as the 8th most bipartisan Senator. More info here:\n",
      "\n",
      " Predicted message type: policy\n",
      "----------------\n",
      "\n",
      "Rep. Ken Buck tweeted: \n",
      "Having only a few hours to read and digest huge bills is an absurd way to run a government. You’d be upset if your teacher assigned War and Peace in the morning and tested you on it later that afternoon. #READIT\n",
      "\n",
      " Predicted message type: policy\n",
      "----------------\n",
      "\n",
      "Rep. Ken Buck tweeted: \n",
      "Please keep in your prayers all of those impacted by the fires burning across our district right now and those who are working to combat these blazes.\n",
      "\n",
      " Predicted message type: personal\n",
      "----------------\n",
      "\n",
      "Sen. Michael Bennet tweeted: \n",
      "Ending TPS for Hondurans forces tens of thousands of law-abiding individuals—many of whom are parents to children who are U.S. citizens—back to a country challenged by violence, poverty, & limited resources. This misguided decision also undermines stability in the region.\n",
      "\n",
      " Predicted message type: policy\n",
      "----------------\n",
      "\n",
      "Sen. Michael Bennet tweeted: \n",
      "The Colorado teacher walkouts are part of a growing movement around the country. Read more on why it's so important that we listen to our teachers:\n",
      "\n",
      " Predicted message type: policy\n",
      "----------------\n",
      "\n",
      "Rep. Jared Polis tweeted: \n",
      "By electing Jared Polis as our next governor, we'll be doing more than breaking another barrier; we'll be sending a fearless, progressive leader to the governor's office.\n",
      "\n",
      " Predicted message type: media\n",
      "----------------\n",
      "\n",
      "Rep. Jared Polis tweeted: \n",
      "Climate change is real & the consequences are becoming a reality. If we want to preserve our CO way of life, and ensure our kids have clean air to breathe, we we can’t afford to wait! As #COGov, I will bring CO to 100% renewable energy by 2040 (or sooner!)\n",
      "\n",
      " Predicted message type: policy\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = [\"ICYMI: Be sure to watch this @FOX21News clip on my recent trip to Afghanistan to visit our Colorado-based troops. I'm incredibly proud of the work these men and women do everyday.\", \n",
    "                 \"My job is to represent Colorado in the United States Senate and that means working across party lines to get things done for the state. I'm proud to be ranked as the 8th most bipartisan Senator. More info here:\",\n",
    "                \"Having only a few hours to read and digest huge bills is an absurd way to run a government. You’d be upset if your teacher assigned War and Peace in the morning and tested you on it later that afternoon. #READIT\", \n",
    "                 \"Please keep in your prayers all of those impacted by the fires burning across our district right now and those who are working to combat these blazes.\",\n",
    "                \"Ending TPS for Hondurans forces tens of thousands of law-abiding individuals—many of whom are parents to children who are U.S. citizens—back to a country challenged by violence, poverty, & limited resources. This misguided decision also undermines stability in the region.\", \n",
    "                 \"The Colorado teacher walkouts are part of a growing movement around the country. Read more on why it's so important that we listen to our teachers:\",\n",
    "                \"By electing Jared Polis as our next governor, we'll be doing more than breaking another barrier; we'll be sending a fearless, progressive leader to the governor's office.\",\n",
    "                \"Climate change is real & the consequences are becoming a reality. If we want to preserve our CO way of life, and ensure our kids have clean air to breathe, we we can’t afford to wait! As #COGov, I will bring CO to 100% renewable energy by 2040 (or sooner!)\"]\n",
    "\n",
    "authors = [\"Sen. Cory Gardner\", \"Sen. Cory Gardner\", \"Rep. Ken Buck\", \"Rep. Ken Buck\", \"Sen. Michael Bennet\", \"Sen. Michael Bennet\", \"Rep. Jared Polis\", \"Rep. Jared Polis\"]\n",
    "pred = tweet_clf.predict(sample_tweets)\n",
    "\n",
    "for i in range(len(sample_tweets)):\n",
    "    print(\"\\n{} tweeted: \\n{}\\n\\n Predicted message type: {}\\n----------------\".format(authors[i], sample_tweets[i], pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classification of these 8 tweets seems fairly correct and matches our intuitions. It signals deep issues in the designation from our qualified human assessors: tweets are not easy to classify. Rarely is a tweet's message black and white, and it largely depends on the point of view of the person classifying the tweet. Our dataset does contain some confidence measurements, depending on whether a tweet was classified by more than one individual, however modifying our model on the basis of these confidence/bias measurements is out of scope of this project. At this time, we can appreciate that our model does a fairly good job of broadly classifying ambiguous tweets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
